# -*- coding: utf-8 -*-
"""Copy of LSDS Intro Assignment 7 - More Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GwL-opmzyONXh43iMmRQDBrmaVAlnGQg

# Lambda School, Intro to Data Science, Day 7 â€” More Regression!

## Assignment

### 1. Experiment with Nearest Neighbor parameter

Using the same 10 training data points from the lesson, train a `KNeighborsRegressor` model with `n_neighbors=1`.

Use both `carat` and `cut` features.

Calculate the mean absolute error on the training data and on the test data.
"""

# %matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error

columns = ['carat', 'cut', 'price']

train = pd.DataFrame(columns=columns, 
        data=[[0.3, 'Ideal', 422],
        [0.31, 'Ideal', 489],
        [0.42, 'Premium', 737],
        [0.5, 'Ideal', 1415],
        [0.51, 'Premium', 1177],
        [0.7, 'Fair', 1865],
        [0.73, 'Fair', 2351],
        [1.01, 'Good', 3768],
        [1.18, 'Very Good', 3965],
        [1.18, 'Ideal', 4838]])

test  = pd.DataFrame(columns=columns, 
        data=[[0.3, 'Ideal', 432],
        [0.34, 'Ideal', 687],
        [0.37, 'Premium', 1124],
        [0.4, 'Good', 720],
        [0.51, 'Ideal', 1397],
        [0.51, 'Very Good', 1284],
        [0.59, 'Ideal', 1437],
        [0.7, 'Ideal', 3419],
        [0.9, 'Premium', 3484],
        [0.9, 'Fair', 2964]])

cut_ranks = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}
train.cut = train.cut.map(cut_ranks)
test.cut = test.cut.map(cut_ranks)

#specify dependent and independent variables
features = ['carat','cut']
target = 'price'

#specify model we are using
model =  KNeighborsRegressor(n_neighbors=2)

#train model
model.fit(train[features], train[target])

#test model
model.fit(test[features], test[target])

#mean absolute error on training model
true_val = train[target]
predict_val = model.predict(train[features])
training_error = mean_absolute_error(true_val, predict_val)

#mean absolute error on test data
true_val2 = test[target]
predict_val2 = model.predict(test[features])
test_error = mean_absolute_error(true_val2, predict_val2)

#display mean absolute error
print('Training error: $', round(training_error))
print('Test error: $', round(test_error), '\n')

"""How does the train error and test error compare to the previous `KNeighborsRegressor` model from the lesson? (The previous model used `n_neighbors=2` and only the `carat` feature.)

Is this new model overfitting or underfitting? Why do you think this is happening here?
"""

#I believe this model is underfitting. I think this is happening because the KNeighbour model does not appropriately capture the trend occuring in this data

"""### 2. More data, two features, linear regression

Use the following code to load data for diamonds under $5,000, and split the data into train and test sets. The training data has almost 30,000 rows, and the test data has almost 10,000 rows.
"""

import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

df = sns.load_dataset('diamonds')
df = df[df.price < 5000]
train, test = train_test_split(df.copy(), random_state=0)
train.shape, test.shape

train

"""Then, train a Linear Regression model with the `carat` and `cut` features. Calculate the mean absolute error on the training data and on the test data."""

#defining the independent and dependent variables
features = ['carat', 'cut']
target = 'price'

#convert cuts into integer
cut_ints = {'Fair':1, 'Good':2, 'Very Good':3, 'Premium':4, 'Ideal':5}
train.cut = train.cut.map(cut_ints)
test.cut = test.cut.map(cut_ints)

#defining the model we are using
model = LinearRegression()

#training the model
model.fit(train[features], train[target])

#training test 
model.fit(test[features], test[target])

#calculate mean absolute value for training data
data = train[target]
predict = model.predict(train[features])
train_error = mean_absolute_error(data, predict)

#calculate mean absolute value for test data
data_2 = test[target]
predict_2 = model.predict(test[features])
test_error = mean_absolute_error(data_2, predict_2)

#print out mean absolutes
print('Training error: $', round(train_error))
print('Test error: $', round(test_error))

"""Use this model to predict the price of a half carat diamond with "very good" cut"""

model.predict([[0.5, 3]])

"""### 3. More data, more features, any model

You choose what features and model type to use! Try to get a better mean absolute error on the test set than your model from the last question.

Refer to [this documentation](https://ggplot2.tidyverse.org/reference/diamonds.html) for more explanation of the features.

Besides `cut`, there are two more ordinal features, which you'd need to encode as numbers if you want to use in your model:
"""

train.describe(include=['object'])

train

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures

clarity_rank = {"IF":0,"VVS1":1, "VVS2":2,"VS1":3, "VS2":4,"SI1":5, "SI2":6, "I1":7}
train.clarity = train.clarity.map(clarity_rank)  

color_rank = {"J":7, "I":6, "H":5, "G":4, "F":3, "E":2, "D":1 }
train.color = train.color.map(color_rank)
test.color = test.color.map(color_rank)

#checking if there are no NAN values popping up
train

#5th degree polynomial
model = make_pipeline(PolynomialFeatures(degree=5), LinearRegression())

features = ['color', 'cut', 'carat']
target = ['price']
model = LinearRegression()
model.fit(train[features],train[target])
model.fit(test[features], test[target])

#mean absolute error
true_val = train[target]
predict = model.predict(train[features])
training_error =  mean_absolute_error(true_val, predict)

#test absolute value
val = test[target]
prediction = model.predict(test[features])
test_error = mean_absolute_error(val, prediction)

#print abs error
print('Training error: $:', round(training_error))
print('Test error: $', round(test_error))
